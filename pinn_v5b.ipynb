{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, math, json, re, time, uuid, random\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from collections import defaultdict\n",
        "\n",
        "# ML libs\n",
        "from astropy.io import fits\n",
        "from astropy.cosmology import Planck18 as cosmo\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_auc_score, brier_score_loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.utils import check_array\n",
        "\n",
        "# nflows (install if missing)\n",
        "try:\n",
        "    from nflows.flows import Flow\n",
        "    from nflows.distributions import StandardNormal\n",
        "    from nflows.transforms import CompositeTransform, MaskedAffineAutoregressiveTransform, ReversePermutation\n",
        "except Exception:\n",
        "    raise RuntimeError(\"nflows is required. Install: pip install nflows\")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1766056607133
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Config & deterministic\n",
        "# ----------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "FEATURES = ['u','g','r','i','z','g-r','u-g','r-i','M_r','redshift']\n",
        "\n",
        "# Quick-run flags (set SUBSET=None to use full data)\n",
        "SUBSET = 10000        # None or int\n",
        "QUICK_EPOCHS_A = 20\n",
        "QUICK_EPOCHS_B = 10\n",
        "QUICK_EPOCHS_C = 10\n",
        "USE_QUICK = True      # toggle quick-run"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cpu\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1766056608733
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Experiment directories\n",
        "# ----------------------\n",
        "ROOT = Path.cwd()\n",
        "BASE_EXP_ROOT = ROOT / \"exp_outputs\"\n",
        "BASE_EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_id = uuid.uuid4().hex[:8]\n",
        "EXP_ROOT = BASE_EXP_ROOT / f\"run_{timestamp}_{run_id}\"\n",
        "EXP_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"EXP_ROOT:\", EXP_ROOT)\n",
        "\n",
        "# ----------------------\n",
        "# Utility functions\n",
        "# ----------------------\n",
        "def metrics(y_true, y_pred):\n",
        "    return {\"rmse\": math.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "            \"mae\": mean_absolute_error(y_true, y_pred),\n",
        "            \"r2\": r2_score(y_true, y_pred)}\n",
        "\n",
        "def fits_to_df_safe(fname, ext=1):\n",
        "    with fits.open(fname, memmap=True) as hdul:\n",
        "        data = hdul[ext].data\n",
        "        names = data.dtype.names\n",
        "        out = {}\n",
        "        for n in names:\n",
        "            col = data[n]\n",
        "            # convert big-endian\n",
        "            if getattr(col, \"dtype\", None) is not None and col.dtype.byteorder in (\">\", \"!\"):\n",
        "                col = col.byteswap().view(col.dtype.newbyteorder(\"=\"))\n",
        "            # handle structured arrays\n",
        "            try:\n",
        "                first = col[0]\n",
        "            except Exception:\n",
        "                out[n] = col\n",
        "                continue\n",
        "            if np.ndim(first) > 0:\n",
        "                arr = np.vstack(col)\n",
        "                for j in range(arr.shape[1]):\n",
        "                    out[f\"{n}_{j}\"] = arr[:, j]\n",
        "            else:\n",
        "                out[n] = col\n",
        "    return pd.DataFrame(out)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "EXP_ROOT: /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1766056630170
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Data load / preprocess\n",
        "# ----------------------\n",
        "\n",
        "# ----------------------\n",
        "# Paths\n",
        "# ----------------------\n",
        "\n",
        "EXP_ROOT = Path(EXP_ROOT)  # ensure Path\n",
        "raw_data_dir = EXP_ROOT / \"raw_data\"\n",
        "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "csv_saved = EXP_ROOT / \"mpa_dr7_clean.csv\"\n",
        "\n",
        "# ----------------------\n",
        "# Load cached CSV if exists\n",
        "# ----------------------\n",
        "\n",
        "if csv_saved.exists():\n",
        "    print(\"Loading saved cleaned CSV:\", csv_saved)\n",
        "    df_final = pd.read_csv(csv_saved)\n",
        "\n",
        "else:\n",
        "    # ----------------------\n",
        "    # Download FITS files (to EXP_ROOT/raw_data)\n",
        "    # ----------------------\n",
        "\n",
        "    base = \"https://wwwmpa.mpa-garching.mpg.de/SDSS/DR7/Data\"\n",
        "    files = {\n",
        "        \"gal_info\": \"gal_info_dr7_v5_2.fit.gz\",\n",
        "        \"gal_sfr\":  \"gal_totsfr_dr7_v5_2.fits.gz\",\n",
        "        \"gal_mass\": \"totlgm_dr7_v5_2.fit.gz\"\n",
        "    }\n",
        "\n",
        "    local_files = {}\n",
        "\n",
        "    for name, fn in files.items():\n",
        "        local_path = raw_data_dir / fn\n",
        "        local_files[name] = local_path\n",
        "\n",
        "        if not local_path.exists():\n",
        "            url = f\"{base}/{fn}\"\n",
        "            print(f\"Downloading {fn} â†’ {local_path}\")\n",
        "            os.system(f\"wget -q {url} -O {local_path}\")\n",
        "        else:\n",
        "            print(\"Exists:\", local_path)\n",
        "\n",
        "    # ----------------------\n",
        "    # Load FITS â†’ DataFrames\n",
        "    # ----------------------\n",
        "\n",
        "    df_info = fits_to_df_safe(local_files[\"gal_info\"])\n",
        "    df_sfr  = fits_to_df_safe(local_files[\"gal_sfr\"])\n",
        "    df_mass = fits_to_df_safe(local_files[\"gal_mass\"])\n",
        "\n",
        "    # Prefix to avoid column collisions\n",
        "    df = pd.concat(\n",
        "        [\n",
        "            df_info,\n",
        "            df_sfr.add_prefix(\"SFR_\"),\n",
        "            df_mass.add_prefix(\"MASS_\")\n",
        "        ],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    print(\"Raw rows:\", len(df))\n",
        "\n",
        "    # ----------------------\n",
        "    # Required column mapping\n",
        "    # ----------------------\n",
        "\n",
        "    required_mapping = {\n",
        "        'u': 'PLUG_MAG_0',\n",
        "        'g': 'PLUG_MAG_1',\n",
        "        'r': 'PLUG_MAG_2',\n",
        "        'i': 'PLUG_MAG_3',\n",
        "        'z': 'PLUG_MAG_4',\n",
        "        'redshift': 'Z',\n",
        "        'logM_true': 'MASS_MEDIAN',\n",
        "        'logSFR_true': 'SFR_AVG'\n",
        "    }\n",
        "\n",
        "    dfc = df.copy()\n",
        "\n",
        "    for out_col, src in required_mapping.items():\n",
        "        if src in dfc.columns:\n",
        "            series = dfc[src]\n",
        "\n",
        "            if series.dtype == object:\n",
        "                def scalarize(x):\n",
        "                    try:\n",
        "                        if hasattr(x, '__len__') and not isinstance(x, (str, bytes)):\n",
        "                            return float(x[0])\n",
        "                        return float(x)\n",
        "                    except Exception:\n",
        "                        return np.nan\n",
        "\n",
        "                dfc[out_col] = series.map(scalarize)\n",
        "            else:\n",
        "                dfc[out_col] = pd.to_numeric(series, errors='coerce')\n",
        "        else:\n",
        "            dfc[out_col] = np.nan\n",
        "\n",
        "    # ----------------------\n",
        "    # Derived colors\n",
        "    # ----------------------\n",
        "\n",
        "    dfc['g-r'] = dfc['g'] - dfc['r']\n",
        "    dfc['u-g'] = dfc['u'] - dfc['g']\n",
        "    dfc['r-i'] = dfc['r'] - dfc['i']\n",
        "\n",
        "    # ----------------------\n",
        "    # Absolute magnitude M_r\n",
        "    # ----------------------\n",
        "\n",
        "    z_valid = (\n",
        "        dfc['redshift'].notna() &\n",
        "        (dfc['redshift'] > 0) &\n",
        "        (dfc['redshift'] < 1.0)\n",
        "    )\n",
        "\n",
        "    dl_pc = np.full(len(dfc), np.nan)\n",
        "\n",
        "    if z_valid.sum() > 0:\n",
        "        dl_pc_vals = cosmo.luminosity_distance(\n",
        "            dfc.loc[z_valid, 'redshift']\n",
        "        ).to('pc').value\n",
        "        dl_pc[z_valid] = dl_pc_vals\n",
        "\n",
        "    dfc['M_r'] = np.nan\n",
        "    mask = (~np.isnan(dl_pc)) & (dl_pc > 0) & dfc['r'].notna()\n",
        "    dfc.loc[mask, 'M_r'] = dfc.loc[mask, 'r'] - 5*np.log10(dl_pc[mask]) + 5\n",
        "\n",
        "    # ----------------------\n",
        "    # Sentinel + numeric cleaning\n",
        "    # ----------------------\n",
        "\n",
        "    for b in ['u', 'g', 'r', 'i', 'z']:\n",
        "        if b in dfc.columns:\n",
        "            dfc.loc[dfc[b] <= -100, b] = np.nan\n",
        "            dfc.loc[dfc[b] < -50, b] = np.nan\n",
        "\n",
        "    dfc = dfc.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    dfc['logM_true'] = pd.to_numeric(dfc['logM_true'], errors='coerce')\n",
        "    dfc['logSFR_true'] = pd.to_numeric(dfc['logSFR_true'], errors='coerce')\n",
        "\n",
        "    # Physical bounds\n",
        "    dfc = dfc[(dfc['logM_true'] >= 5) & (dfc['logM_true'] <= 13)]\n",
        "    dfc = dfc[(dfc['logSFR_true'] >= -5) & (dfc['logSFR_true'] <= 2)]\n",
        "\n",
        "    # ----------------------\n",
        "    # Final selection\n",
        "    # ----------------------\n",
        "\n",
        "    essential = [\n",
        "        'u','g','r','i','z',\n",
        "        'g-r','u-g','r-i',\n",
        "        'M_r','redshift',\n",
        "        'logM_true','logSFR_true'\n",
        "    ]\n",
        "\n",
        "    df_final = (\n",
        "        dfc\n",
        "        .dropna(subset=essential)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    csv_saved.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df_final.to_csv(csv_saved, index=False)\n",
        "\n",
        "    print(\"Saved cleaned CSV:\", csv_saved)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading gal_info_dr7_v5_2.fit.gz â†’ /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2/raw_data/gal_info_dr7_v5_2.fit.gz\nDownloading gal_totsfr_dr7_v5_2.fits.gz â†’ /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2/raw_data/gal_totsfr_dr7_v5_2.fits.gz\nDownloading totlgm_dr7_v5_2.fit.gz â†’ /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2/raw_data/totlgm_dr7_v5_2.fit.gz\nRaw rows: 927552\nSaved cleaned CSV: /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2/mpa_dr7_clean.csv\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1766056824682
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Arrays & splits\n",
        "# ----------------------\n",
        "X = df_final[FEATURES].values.astype(np.float32)\n",
        "yM = df_final['logM_true'].values.astype(np.float32)\n",
        "yS = df_final['logSFR_true'].values.astype(np.float32)\n",
        "\n",
        "idx_all = np.arange(len(X))\n",
        "idx_temp, idx_test = train_test_split(idx_all, test_size=0.20, random_state=SEED)\n",
        "idx_train, idx_val = train_test_split(idx_temp, test_size=0.20, random_state=SEED)\n",
        "\n",
        "X_train, X_val, X_test = X[idx_train], X[idx_val], X[idx_test]\n",
        "yM_train, yM_val, yM_test = yM[idx_train], yM[idx_val], yM[idx_test]\n",
        "yS_train, yS_val, yS_test = yS[idx_train], yS[idx_val], yS[idx_test]\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_s = scaler.transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "joblib.dump(scaler, os.path.join(EXP_ROOT, \"scaler.joblib\"))\n",
        "print(\"Shapes:\", X_train_s.shape, X_val_s.shape, X_test_s.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Shapes: (498046, 10) (124512, 10) (155640, 10)\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1766056825247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Quick-run subset override (safe)\n",
        "# ----------------------\n",
        "if USE_QUICK and (SUBSET is not None):\n",
        "    sel = np.random.RandomState(SEED).choice(np.arange(len(X_train_s)), size=min(SUBSET, len(X_train_s)), replace=False)\n",
        "    X_train_s = X_train_s[sel]\n",
        "    yM_train = yM_train[sel]\n",
        "    yS_train = yS_train[sel]\n",
        "    print(\"Quick-run active. Train subset size:\", X_train_s.shape[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Quick-run active. Train subset size: 10000\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1766056825636
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% \n",
        "# Cell 5 (REPLACEMENT) â€” robust priors with GMM clustering & RANSAC (works with subset helper)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# --- Determine active training feature matrix in original (unscaled) units ---\n",
        "# We want X_train_unscaled to line up with yM_train length (handles subset scenario).\n",
        "if 'X_train' in globals() and X_train.shape[0] == yM_train.shape[0]:\n",
        "    X_train_unscaled = X_train\n",
        "elif 'X_train_s' in globals() and X_train_s.shape[0] == yM_train.shape[0] and 'scaler' in globals() and scaler is not None:\n",
        "    # inverse transform the scaled subset back to original units\n",
        "    X_train_unscaled = scaler.inverse_transform(X_train_s)\n",
        "elif 'X' in globals() and 'idx_train' in globals():\n",
        "    # fallback: select rows from full X by idx_train (useable when subset helper wasn't used)\n",
        "    X_train_unscaled = X[np.array(idx_train)]\n",
        "    if X_train_unscaled.shape[0] != yM_train.shape[0]:\n",
        "        raise RuntimeError(\"Fallback selection produced mismatched sizes. Re-run data split (Cell 4).\")\n",
        "else:\n",
        "    raise RuntimeError(\"Cannot infer training feature matrix matching yM_train length. Re-run Cell 4 or ensure subset helper didn't partially mutate arrays.\")\n",
        "\n",
        "print(\"GMM / priors will be computed on training rows:\", X_train_unscaled.shape[0])\n",
        "\n",
        "# --- GMM on (logM, logSFR) using the active training targets ---\n",
        "XY_train = np.vstack([yM_train, yS_train]).T\n",
        "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=SEED)\n",
        "gmm.fit(XY_train)\n",
        "labels = gmm.predict(XY_train)\n",
        "comp_means = [XY_train[labels==c,1].mean() if (labels==c).sum()>0 else -1e9 for c in [0,1]]\n",
        "sf_label = int(np.argmax(comp_means))\n",
        "print(\"GMM component mean logSFRs:\", comp_means, \"-> SF label:\", sf_label)\n",
        "\n",
        "# prepare cluster dictionary\n",
        "clusters = {}\n",
        "for comp in [0,1]:\n",
        "    mask_comp = (labels == comp)\n",
        "    n_comp = int(mask_comp.sum())\n",
        "    # if component too small, fall back to full train (avoid unstable fits)\n",
        "    if n_comp < 10:\n",
        "        idx_comp_mask = np.ones(len(yM_train), dtype=bool)\n",
        "        print(f\"Warning: cluster {comp} too small ({n_comp}). Falling back to full train.\")\n",
        "    else:\n",
        "        idx_comp_mask = mask_comp\n",
        "\n",
        "    pred_cols = ['g-r', 'u-g', 'M_r', 'redshift']\n",
        "    # build design Xp from X_train_unscaled aligned with yM_train ordering\n",
        "    Xp = np.vstack([X_train_unscaled[idx_comp_mask, FEATURES.index(c)] for c in pred_cols]).T\n",
        "    y_comp_mass = yM_train[idx_comp_mask]\n",
        "\n",
        "    # robust RANSAC fit for mass multi-linear prior\n",
        "    base_lr = LinearRegression()\n",
        "    ransac = RANSACRegressor(estimator=base_lr, min_samples=0.5, residual_threshold=1.0, random_state=SEED)\n",
        "    ransac.fit(Xp, y_comp_mass)\n",
        "    coef = ransac.estimator_.coef_.astype(float)\n",
        "    intercept = float(ransac.estimator_.intercept_)\n",
        "\n",
        "    # MS prior: logSFR ~ A + B * logM  (robust via RANSAC)\n",
        "    Xm = yM_train[idx_comp_mask].reshape(-1,1)\n",
        "    ys_comp = yS_train[idx_comp_mask]\n",
        "    ransac_ms = RANSACRegressor(estimator=LinearRegression(), min_samples=0.5, residual_threshold=0.5, random_state=SEED)\n",
        "    ransac_ms.fit(Xm, ys_comp)\n",
        "    A = float(ransac_ms.estimator_.intercept_)\n",
        "    B = float(ransac_ms.estimator_.coef_[0])\n",
        "\n",
        "    clusters[f\"cluster_{comp}\"] = {\n",
        "        \"n_train\": int(n_comp),\n",
        "        \"mass_multi\": {\"intercept\": intercept, \"coeffs\": dict(zip(pred_cols, coef.tolist())), \"predictor_order\": pred_cols},\n",
        "        \"MS\": {\"A\": A, \"B\": B}\n",
        "    }\n",
        "\n",
        "# --- Compatibility 2-term prior on the currently active training set (g-r, M_r) ---\n",
        "X2 = np.vstack([\n",
        "    X_train_unscaled[:, FEATURES.index('g-r')],\n",
        "    X_train_unscaled[:, FEATURES.index('M_r')]\n",
        "]).T\n",
        "lr2 = LinearRegression().fit(X2, yM_train)\n",
        "a2 = float(lr2.intercept_)\n",
        "# coef_ may be 1d length-2\n",
        "b2, c2 = float(lr2.coef_[0]), float(lr2.coef_[1])\n",
        "print(\"Compatibility 2-term prior fitted on rows:\", X_train_unscaled.shape[0], \"a2,b2,c2:\", a2, b2, c2)\n",
        "\n",
        "# --- Overall robust prior on whole (active) training set using RANSAC too ---\n",
        "def fit_mass_prior_robust(X_unscaled, y_arr):\n",
        "    pred_cols = ['g-r','u-g','M_r','redshift']\n",
        "    X_train_mass = np.vstack([X_unscaled[:, FEATURES.index(c)] for c in pred_cols]).T\n",
        "    y_train_mass = y_arr\n",
        "    base_lr = LinearRegression()\n",
        "    ransac = RANSACRegressor(estimator=base_lr, min_samples=0.5, residual_threshold=1.0, random_state=SEED)\n",
        "    ransac.fit(X_train_mass, y_train_mass)\n",
        "    coef = ransac.estimator_.coef_.astype(float)\n",
        "    intercept = float(ransac.estimator_.intercept_)\n",
        "    return intercept, dict(zip(pred_cols, coef.tolist())), pred_cols\n",
        "\n",
        "intercept_all, coeffs_all, pred_order_all = fit_mass_prior_robust(X_train_unscaled, yM_train)\n",
        "\n",
        "# average MS A,B across clusters (safe if both clusters present)\n",
        "A_all = float((clusters['cluster_0']['MS']['A'] + clusters['cluster_1']['MS']['A'])/2.0)\n",
        "B_all = float((clusters['cluster_0']['MS']['B'] + clusters['cluster_1']['MS']['B'])/2.0)\n",
        "\n",
        "priors = {\n",
        "    \"clusters\": clusters,\n",
        "    \"mass_multi\": {\"intercept\": intercept_all, \"coeffs\": coeffs_all, \"predictor_order\": pred_order_all, \"n_train\": int(X_train_unscaled.shape[0])},\n",
        "    \"a\": a2, \"b\": float(b2), \"c\": float(c2),\n",
        "    \"MS_overall\": {\"A\": A_all, \"B\": B_all},\n",
        "}\n",
        "\n",
        "# save\n",
        "os.makedirs(EXP_ROOT, exist_ok=True)\n",
        "with open(os.path.join(EXP_ROOT, \"priors.json\"), \"w\") as f:\n",
        "    json.dump(priors, f, indent=2)\n",
        "\n",
        "print(\"Saved clustered priors. cluster sizes (train):\", {k: v['n_train'] for k, v in clusters.items()})\n",
        "print(\"Saved priors.json to\", os.path.join(EXP_ROOT, \"priors.json\"))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "GMM / priors will be computed on training rows: 10000\nGMM component mean logSFRs: [np.float32(0.17674802), np.float32(-0.81718224)] -> SF label: 0\nCompatibility 2-term prior fitted on rows: 10000 a2,b2,c2: 3.53395938873291 0.8389224410057068 -0.3178345859050751\nSaved clustered priors. cluster sizes (train): {'cluster_0': 5545, 'cluster_1': 4455}\nSaved priors.json to /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2/priors.json\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1766056826007
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Cell 6 â€” Baseline RFs\n",
        "# ----------------------\n",
        "rf_params = dict(n_estimators=50, max_depth=20, n_jobs=1, random_state=SEED)\n",
        "print(\"Training RF with params:\", rf_params)\n",
        "rf_m = RandomForestRegressor(**rf_params)\n",
        "rf_s = RandomForestRegressor(**rf_params)\n",
        "t0 = time.time()\n",
        "rf_m.fit(X_train_s, yM_train)\n",
        "rf_s.fit(X_train_s, yS_train)\n",
        "t_rf = time.time() - t0\n",
        "print(\"RF training time (s):\", t_rf)\n",
        "m_rf_pred = rf_m.predict(X_test_s)\n",
        "s_rf_pred = rf_s.predict(X_test_s)\n",
        "print(\"RF mass metrics:\", metrics(yM_test, m_rf_pred))\n",
        "print(\"RF sfr metrics:\", metrics(yS_test, s_rf_pred))\n",
        "joblib.dump(rf_m, os.path.join(EXP_ROOT, \"rf_mass.joblib\"))\n",
        "joblib.dump(rf_s, os.path.join(EXP_ROOT, \"rf_sfr.joblib\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training RF with params: {'n_estimators': 50, 'max_depth': 20, 'n_jobs': 1, 'random_state': 42}\nRF training time (s): 11.857103824615479\nRF mass metrics: {'rmse': 0.18910025399679853, 'mae': 0.12695345559779875, 'r2': 0.9277469099066622}\nRF sfr metrics: {'rmse': 0.3658942826194197, 'mae': 0.2642528547539208, 'r2': 0.7532198399923205}\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "['/mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2/rf_sfr.joblib']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1766056841575
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Stage A â€” deterministic multitask PINN (pretrain)\n",
        "# ----------------------\n",
        "class MTNN_det(nn.Module):\n",
        "    \"\"\"\n",
        "    Stage A: Deterministic Physics-Informed Neural Network\n",
        "    Predicts physically bounded logM and logSFR\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Shared encoder\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Raw heads (bounded via tanh)\n",
        "        self.m_head = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.s_head = nn.Sequential(\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Physical bounds\n",
        "        # logM âˆˆ [5, 13] â†’ center=9, half-range=4\n",
        "        self.M_CENTER = 9.0\n",
        "        self.M_SCALE  = 4.0\n",
        "\n",
        "        # logSFR âˆˆ [-5, 2] â†’ center=-1.5, half-range=3.5\n",
        "        self.S_CENTER = -1.5\n",
        "        self.S_SCALE  = 3.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "\n",
        "        # Raw bounded outputs in (-1, 1)\n",
        "        m_raw = self.m_head(h).squeeze(-1)\n",
        "        s_raw = self.s_head(h).squeeze(-1)\n",
        "\n",
        "        # Rescale to physical ranges\n",
        "        m_mu = self.M_CENTER + self.M_SCALE * m_raw\n",
        "        s_mu = self.S_CENTER + self.S_SCALE * s_raw\n",
        "\n",
        "        return m_mu, s_mu\n",
        "\n",
        "batch_A = 512\n",
        "train_ds = TensorDataset(torch.tensor(X_train_s, dtype=torch.float32),\n",
        "                         torch.tensor(yM_train, dtype=torch.float32),\n",
        "                         torch.tensor(yS_train, dtype=torch.float32))\n",
        "val_ds = TensorDataset(torch.tensor(X_val_s, dtype=torch.float32),\n",
        "                       torch.tensor(yM_val, dtype=torch.float32),\n",
        "                       torch.tensor(yS_val, dtype=torch.float32))\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_A, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=1024, shuffle=False)\n",
        "\n",
        "modelA = MTNN_det(X_train_s.shape[1]).to(DEVICE)\n",
        "optA = Adam(modelA.parameters(), lr=5e-4, weight_decay=1e-6)\n",
        "epochsA = QUICK_EPOCHS_A if USE_QUICK else 50\n",
        "clip_norm = 5.0\n",
        "\n",
        "# load priors\n",
        "with open(os.path.join(EXP_ROOT, \"priors.json\"), \"r\") as f:\n",
        "    pri = json.load(f)\n",
        "pri_a = float(pri.get('a', 0.0)); pri_b = float(pri.get('b', 0.0)); pri_c = float(pri.get('c', 0.0))\n",
        "pri_A = float(pri.get('MS_overall', {}).get('A', 0.0)); pri_B = float(pri.get('MS_overall', {}).get('B', 0.0))\n",
        "LAMBDA_MASS = 0.02; LAMBDA_MS = 0.02\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "best_val = 1e12; best_stateA = None\n",
        "for epoch in range(1, epochsA+1):\n",
        "    modelA.train()\n",
        "    losses = []\n",
        "    for Xb, ym, ys in train_loader:\n",
        "        Xb = Xb.to(DEVICE); ym = ym.to(DEVICE); ys = ys.to(DEVICE)\n",
        "        m_p, s_p = modelA(Xb)\n",
        "        L_data = mse_loss(m_p, ym) + mse_loss(s_p, ys)\n",
        "        # mass prior computed on original scale\n",
        "        Xb_un = scaler.inverse_transform(Xb.cpu().numpy())\n",
        "        mass_prior = pri_a + pri_b * Xb_un[:, FEATURES.index('g-r')] + pri_c * Xb_un[:, FEATURES.index('M_r')]\n",
        "        mass_prior_t = torch.tensor(mass_prior, dtype=torch.float32, device=DEVICE)\n",
        "        L_mass = mse_loss(m_p, mass_prior_t)\n",
        "        L_MS = mse_loss(s_p, pri_A + pri_B * m_p.detach())\n",
        "        L_total = L_data + LAMBDA_MASS * L_mass + LAMBDA_MS * L_MS\n",
        "        optA.zero_grad(); L_total.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(modelA.parameters(), clip_norm)\n",
        "        optA.step()\n",
        "        losses.append(float(L_total.detach().cpu().numpy()))\n",
        "    # val\n",
        "    modelA.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = []; trues = []\n",
        "        for Xb, ym, ys in val_loader:\n",
        "            Xb = Xb.to(DEVICE)\n",
        "            m_p, s_p = modelA(Xb)\n",
        "            preds.append(m_p.cpu().numpy()); trues.append(ym.numpy())\n",
        "    val_mse = mean_squared_error(np.concatenate(trues), np.concatenate(preds))\n",
        "    if val_mse < best_val:\n",
        "        best_val = val_mse\n",
        "        best_stateA = {k:v.cpu() for k,v in modelA.state_dict().items()}\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"[Stage A] Epoch {epoch:03d} train_loss={np.mean(losses):.6f} val_mse={val_mse:.6f}\")\n",
        "\n",
        "torch.save(best_stateA, os.path.join(EXP_ROOT,\"pinn_stageA_best.pth\"))\n",
        "print(\"Saved Stage A best.\")\n",
        "\n",
        "# Build Context wrapper from Stage A (only if Stage A actually trained)\n",
        "class ContextNetWrapper(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(dim,256), nn.ReLU(),\n",
        "            nn.Linear(256,128), nn.ReLU()\n",
        "        )\n",
        "        self.context_net = nn.Sequential(\n",
        "            nn.Linear(128,64), nn.ReLU(),\n",
        "            nn.Linear(64,64), nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.context_net(self.encoder(x))\n",
        "\n",
        "ctx_model = ContextNetWrapper(X_train_s.shape[1]).to(DEVICE)\n",
        "\n",
        "if best_stateA is not None:\n",
        "    # Map weights from modelA.shared -> ctx_model.encoder\n",
        "    sdA = best_stateA\n",
        "    mapping = {\n",
        "        'encoder.0.weight':'shared.0.weight',\n",
        "        'encoder.0.bias':'shared.0.bias',\n",
        "        'encoder.2.weight':'shared.2.weight',\n",
        "        'encoder.2.bias':'shared.2.bias'\n",
        "    }\n",
        "    sd_ctx = ctx_model.state_dict()\n",
        "    for t_key, s_key in mapping.items():\n",
        "        if s_key in sdA and t_key in sd_ctx:\n",
        "            sd_ctx[t_key] = sdA[s_key]\n",
        "    ctx_model.load_state_dict(sd_ctx)\n",
        "    for p in ctx_model.parameters():\n",
        "        p.requires_grad = False\n",
        "    print(\"Context model prepared from Stage A weights (frozen).\")\n",
        "else:\n",
        "    # Stage A was skipped; leave context model randomly initialized\n",
        "    print(\"Stage A skipped; context model initialized randomly.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[Stage A] Epoch 001 train_loss=2.105799 val_mse=0.225429\n[Stage A] Epoch 005 train_loss=0.201544 val_mse=0.038739\n[Stage A] Epoch 010 train_loss=0.182673 val_mse=0.040709\n[Stage A] Epoch 015 train_loss=0.178869 val_mse=0.037964\n[Stage A] Epoch 020 train_loss=0.169640 val_mse=0.036103\nSaved Stage A best.\nContext model prepared from Stage A weights (frozen).\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1766056881497
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Stage A evaluation ONLY =====\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "modelA.eval()\n",
        "with torch.no_grad():\n",
        "    X_t = torch.tensor(X_test_s, dtype=torch.float32).to(DEVICE)\n",
        "    m_mu, s_mu = modelA(X_t)\n",
        "\n",
        "mu_m = m_mu.cpu().numpy()\n",
        "mu_s = s_mu.cpu().numpy()\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"rmse\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
        "        \"r2\": r2_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "print(\"Stage A mass metrics:\", metrics(yM_test, mu_m))\n",
        "print(\"Stage A sfr metrics :\", metrics(yS_test, mu_s))\n",
        "print(\"mu_m range:\", mu_m.min(), mu_m.max())\n",
        "print(\"mu_s range:\", mu_s.min(), mu_s.max())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Stage A mass metrics: {'rmse': np.float64(0.18765991067611226), 'mae': 0.12187053263187408, 'r2': 0.9288433790206909}\nStage A sfr metrics : {'rmse': np.float64(0.35468255598344756), 'mae': 0.2568613588809967, 'r2': 0.7681117653846741}\nmu_m range: 5.40917 12.80234\nmu_s range: -4.767842 1.9999955\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1766056882033
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Stage B â€” flow-only training (conditional flow for Q)\n",
        "# ----------------------\n",
        "\n",
        "def build_conditional_maf(context_dim, n_blocks=4, hidden_features=64):\n",
        "    transforms = []\n",
        "    for _ in range(n_blocks):\n",
        "        transforms.append(\n",
        "            MaskedAffineAutoregressiveTransform(\n",
        "                features=1,\n",
        "                hidden_features=hidden_features,\n",
        "                context_features=context_dim,\n",
        "                num_blocks=2,\n",
        "                use_residual_blocks=False\n",
        "            )\n",
        "        )\n",
        "        transforms.append(ReversePermutation(features=1))\n",
        "    transform = CompositeTransform(transforms)\n",
        "    base_dist = StandardNormal([1])\n",
        "    return Flow(transform, base_dist)\n",
        "\n",
        "\n",
        "# ---- Instantiate flow ----\n",
        "context_dim = 64\n",
        "flow = build_conditional_maf(\n",
        "    context_dim=context_dim,\n",
        "    n_blocks=4,\n",
        "    hidden_features=64\n",
        ").to(DEVICE)\n",
        "\n",
        "opt_flow = Adam(flow.parameters(), lr=3e-4, weight_decay=1e-6)\n",
        "epochsB = QUICK_EPOCHS_B if USE_QUICK else 40\n",
        "\n",
        "# ---- Identify quenched component from GMM (fit earlier on train) ----\n",
        "comp_means_sfr = gmm.means_[:, 1]\n",
        "quenched_comp = int(np.argmin(comp_means_sfr))\n",
        "print(\"GMM train means (logSFR):\", comp_means_sfr, \"-> quenched:\", quenched_comp)\n",
        "\n",
        "train_loader_flow = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "\n",
        "best_flow_state = None\n",
        "best_flow_loss = float(\"inf\")\n",
        "\n",
        "# ----------------------\n",
        "# Training loop\n",
        "# ----------------------\n",
        "for epoch in range(1, epochsB + 1):\n",
        "    flow.train()\n",
        "    losses = []\n",
        "\n",
        "    for Xb, ym, ys in train_loader_flow:\n",
        "        Xb = Xb.to(DEVICE).float()\n",
        "\n",
        "        # ---- Context from frozen encoder ----\n",
        "        with torch.no_grad():\n",
        "            ctx = ctx_model(Xb)\n",
        "        ctx = ctx.detach()  # ðŸ”’ critical: prevent any leakage / gradients\n",
        "\n",
        "        # ---- Soft quenching target from GMM ----\n",
        "        ym_np = ym.numpy()\n",
        "        ys_np = ys.numpy()\n",
        "        q_target = gmm.predict_proba(\n",
        "            np.vstack([ym_np, ys_np]).T\n",
        "        )[:, quenched_comp].astype(np.float32)\n",
        "\n",
        "        q_t = torch.tensor(q_target, device=DEVICE).unsqueeze(-1)\n",
        "\n",
        "        # ---- Flow NLL ----\n",
        "        nll = -flow.log_prob(q_t, context=ctx).mean()\n",
        "\n",
        "        opt_flow.zero_grad()\n",
        "        nll.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(flow.parameters(), 5.0)\n",
        "        opt_flow.step()\n",
        "\n",
        "        losses.append(float(nll.detach().cpu()))\n",
        "\n",
        "    avg_nll = float(np.mean(losses))\n",
        "    if avg_nll < best_flow_loss:\n",
        "        best_flow_loss = avg_nll\n",
        "        best_flow_state = {k: v.cpu() for k, v in flow.state_dict().items()}\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"[Stage B] Epoch {epoch:03d} flow_nll={avg_nll:.6f}\")\n",
        "\n",
        "# ----------------------\n",
        "# Save best flow\n",
        "# ----------------------\n",
        "if best_flow_state is not None:\n",
        "    torch.save(\n",
        "        best_flow_state,\n",
        "        os.path.join(EXP_ROOT, \"pinn_flow_stageB_flow.pth\")\n",
        "    )\n",
        "    print(\"Saved Stage B flow.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "GMM train means (logSFR): [ 0.17254205 -0.81283045] -> quenched: 1\n[Stage B] Epoch 001 flow_nll=2.053684\n[Stage B] Epoch 005 flow_nll=0.653947\n[Stage B] Epoch 010 flow_nll=0.147519\nSaved Stage B flow.\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1766056887233
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Cell 7C â€” Stage C: Joint fine-tune (bounded, safe, copy-paste ready)\n",
        "import os, math, json, numpy as np, random, torch, torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Deterministic seed\n",
        "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Stage C running on device:\", DEVICE)\n",
        "\n",
        "# --- Fallback: ensure loaders exist ---\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "if 'train_loader' not in globals() or 'val_loader' not in globals():\n",
        "    print(\"Fallback: creating train/val DataLoaders from arrays.\")\n",
        "    batch_train = globals().get('BATCH_TRAIN', 512)\n",
        "    batch_val = globals().get('BATCH_VAL', 1024)\n",
        "    train_ds = TensorDataset(torch.tensor(X_train_s,dtype=torch.float32),\n",
        "                             torch.tensor(yM_train,dtype=torch.float32),\n",
        "                             torch.tensor(yS_train,dtype=torch.float32))\n",
        "    val_ds = TensorDataset(torch.tensor(X_val_s,dtype=torch.float32),\n",
        "                           torch.tensor(yM_val,dtype=torch.float32),\n",
        "                           torch.tensor(yS_val,dtype=torch.float32))\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_train, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_val, shuffle=False)\n",
        "    print(\"Created fallback loaders. Train size:\", len(train_ds), \"Val size:\", len(val_ds))\n",
        "\n",
        "# ----------------------\n",
        "# Joint model (BOUNDED means)\n",
        "# ----------------------\n",
        "class PINNJoint(nn.Module):\n",
        "    def __init__(self, dim, context_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Shared encoder (same as Stage A)\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(dim,256), nn.ReLU(),\n",
        "            nn.Linear(256,128), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Raw bounded heads\n",
        "        self.m_head = nn.Sequential(nn.Linear(128,1), nn.Tanh())\n",
        "        self.s_head = nn.Sequential(nn.Linear(128,1), nn.Tanh())\n",
        "\n",
        "        # Heteroscedastic log-variance heads\n",
        "        self.m_logvar = nn.Linear(128,1)\n",
        "        self.s_logvar = nn.Linear(128,1)\n",
        "\n",
        "        # Context net for flow\n",
        "        self.context_net = nn.Sequential(\n",
        "            nn.Linear(128,context_dim), nn.ReLU(),\n",
        "            nn.Linear(context_dim,context_dim), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Physical bounds (same as Stage A)\n",
        "        # logM âˆˆ [5, 13]\n",
        "        self.M_CENTER = 9.0\n",
        "        self.M_SCALE  = 4.0\n",
        "        # logSFR âˆˆ [-5, 2]\n",
        "        self.S_CENTER = -1.5\n",
        "        self.S_SCALE  = 3.5\n",
        "\n",
        "        self.context_dim = context_dim\n",
        "        self.flow = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.shared(x)\n",
        "\n",
        "        # bounded means\n",
        "        m_raw = self.m_head(h).squeeze(-1)\n",
        "        s_raw = self.s_head(h).squeeze(-1)\n",
        "\n",
        "        m_mu = self.M_CENTER + self.M_SCALE * m_raw\n",
        "        s_mu = self.S_CENTER + self.S_SCALE * s_raw\n",
        "\n",
        "        # log-variances\n",
        "        m_logvar = self.m_logvar(h).squeeze(-1)\n",
        "        s_logvar = self.s_logvar(h).squeeze(-1)\n",
        "\n",
        "        ctx = self.context_net(h)\n",
        "\n",
        "        return {\n",
        "            \"context\": ctx,\n",
        "            \"mu_mass\": m_mu,\n",
        "            \"logvar_mass\": m_logvar,\n",
        "            \"mu_sfr\": s_mu,\n",
        "            \"logvar_sfr\": s_logvar\n",
        "        }\n",
        "\n",
        "# ----------------------\n",
        "# Flow builder\n",
        "# ----------------------\n",
        "def build_conditional_maf(context_dim, n_blocks=6, hidden_features=128):\n",
        "    from nflows.flows import Flow\n",
        "    from nflows.distributions import StandardNormal\n",
        "    from nflows.transforms import CompositeTransform, MaskedAffineAutoregressiveTransform, ReversePermutation\n",
        "    transforms = []\n",
        "    for _ in range(n_blocks):\n",
        "        transforms.append(MaskedAffineAutoregressiveTransform(\n",
        "            features=1,\n",
        "            hidden_features=hidden_features,\n",
        "            context_features=context_dim,\n",
        "            num_blocks=2,\n",
        "            use_residual_blocks=False\n",
        "        ))\n",
        "        transforms.append(ReversePermutation(features=1))\n",
        "    return Flow(CompositeTransform(transforms), StandardNormal([1]))\n",
        "\n",
        "# ----------------------\n",
        "# Instantiate joint model\n",
        "# ----------------------\n",
        "context_dim = 64\n",
        "joint = PINNJoint(X_train_s.shape[1], context_dim=context_dim).to(DEVICE)\n",
        "\n",
        "# --- Load Stage A weights (shared encoder only) ---\n",
        "p_stageA = os.path.join(EXP_ROOT, \"pinn_stageA_best.pth\")\n",
        "if os.path.exists(p_stageA):\n",
        "    sdA = torch.load(p_stageA, map_location='cpu')\n",
        "    joint_sd = joint.state_dict()\n",
        "    copied = 0\n",
        "    for k,v in sdA.items():\n",
        "        if k in joint_sd and joint_sd[k].shape == v.shape:\n",
        "            joint_sd[k].copy_(v)\n",
        "            copied += 1\n",
        "    joint.load_state_dict(joint_sd)\n",
        "    print(f\"Loaded Stage A weights (copied {copied} params).\")\n",
        "\n",
        "# --- Load Stage B flow (infer architecture if needed) ---\n",
        "p_flowB = os.path.join(EXP_ROOT, \"pinn_flow_stageB_flow.pth\")\n",
        "flow_loaded = False\n",
        "if os.path.exists(p_flowB):\n",
        "    try:\n",
        "        joint.flow = build_conditional_maf(context_dim, n_blocks=4, hidden_features=64).to(DEVICE)\n",
        "        joint.flow.load_state_dict(torch.load(p_flowB, map_location='cpu'))\n",
        "        flow_loaded = True\n",
        "        print(\"Loaded Stage B flow.\")\n",
        "    except Exception as e:\n",
        "        print(\"Flow load issue:\", e)\n",
        "\n",
        "if not flow_loaded:\n",
        "    joint.flow = build_conditional_maf(context_dim, n_blocks=4, hidden_features=64).to(DEVICE)\n",
        "    print(\"Using randomly initialized flow.\")\n",
        "\n",
        "# ----------------------\n",
        "# Optimizer & config\n",
        "# ----------------------\n",
        "for p in joint.parameters(): p.requires_grad = True\n",
        "for p in joint.flow.parameters(): p.requires_grad = True\n",
        "\n",
        "opt = Adam(list(joint.parameters()) + list(joint.flow.parameters()), lr=1e-4, weight_decay=1e-6)\n",
        "epochsC = globals().get('epochsC', 20)\n",
        "clip_norm = 5.0\n",
        "\n",
        "LOGVAR_MIN, LOGVAR_MAX = -10.0, 5.0\n",
        "\n",
        "def gaussian_nll_torch(y, mu, logvar):\n",
        "    logvar = torch.clamp(logvar, LOGVAR_MIN, LOGVAR_MAX)\n",
        "    var = torch.exp(logvar)\n",
        "    return 0.5 * ((y - mu)**2 / var + logvar + math.log(2*math.pi))\n",
        "\n",
        "# ----------------------\n",
        "# Priors & GMM\n",
        "# ----------------------\n",
        "with open(os.path.join(EXP_ROOT, \"priors.json\"), \"r\") as f:\n",
        "    pri = json.load(f)\n",
        "\n",
        "pri_a = float(pri.get('a',0)); pri_b = float(pri.get('b',0)); pri_c = float(pri.get('c',0))\n",
        "A_all = float(pri.get('MS_overall',{}).get('A',0))\n",
        "B_all = float(pri.get('MS_overall',{}).get('B',0))\n",
        "\n",
        "LAMBDA_MASS = 0.02; LAMBDA_MS = 0.02; LAMBDA_Q = 1.0; LAMBDA_Q_PHYS = 0.5\n",
        "\n",
        "gmm_train = GaussianMixture(n_components=2, covariance_type='full', random_state=SEED)\n",
        "gmm_train.fit(np.vstack([yM_train, yS_train]).T)\n",
        "quenched_comp = int(np.argmin(gmm_train.means_[:,1]))\n",
        "\n",
        "# ----------------------\n",
        "# Training loop (uncomment to run)\n",
        "# ----------------------\n",
        "best_val = 1e12; best_state = None; best_flow_state = None\n",
        "\n",
        "for epoch in range(1, epochsC+1):\n",
        "    joint.train(); joint.flow.train()\n",
        "    losses = []\n",
        "    for Xb, ym, ys in train_loader:\n",
        "        Xb = Xb.to(DEVICE); ym = ym.to(DEVICE); ys = ys.to(DEVICE)\n",
        "        out = joint(Xb)\n",
        "        ctx = out[\"context\"]\n",
        "        m_mu = out[\"mu_mass\"]; s_mu = out[\"mu_sfr\"]\n",
        "        m_lv = torch.clamp(out[\"logvar_mass\"], LOGVAR_MIN, LOGVAR_MAX)\n",
        "        s_lv = torch.clamp(out[\"logvar_sfr\"], LOGVAR_MIN, LOGVAR_MAX)\n",
        "\n",
        "        L_data = gaussian_nll_torch(ym, m_mu, m_lv).mean() + gaussian_nll_torch(ys, s_mu, s_lv).mean()\n",
        "\n",
        "        Xb_un = scaler.inverse_transform(Xb.cpu().numpy())\n",
        "        mass_prior = pri_a + pri_b*Xb_un[:,FEATURES.index('g-r')] + pri_c*Xb_un[:,FEATURES.index('M_r')]\n",
        "        L_mass = nn.functional.mse_loss(m_mu, torch.tensor(mass_prior, device=DEVICE))\n",
        "        L_MS = nn.functional.mse_loss(s_mu, A_all + B_all*m_mu.detach())\n",
        "\n",
        "        q_t = torch.tensor(\n",
        "            gmm_train.predict_proba(np.vstack([ym.cpu().numpy(), ys.cpu().numpy()]).T)[:,quenched_comp],\n",
        "            device=DEVICE\n",
        "        ).unsqueeze(-1)\n",
        "\n",
        "        nll_q = -joint.flow.log_prob(q_t, context=ctx.detach()).mean()\n",
        "\n",
        "        L = L_data + LAMBDA_MASS*L_mass + LAMBDA_MS*L_MS + LAMBDA_Q*nll_q\n",
        "        opt.zero_grad(); L.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(list(joint.parameters())+list(joint.flow.parameters()), clip_norm)\n",
        "        opt.step()\n",
        "        losses.append(L.item())\n",
        "\n",
        "    print(f\"[Stage C] Epoch {epoch:03d} loss={np.mean(losses):.4f}\")\n",
        "\n",
        "# ----------------------\n",
        "# Save FINAL Stage C model (REQUIRED)\n",
        "# ----------------------\n",
        "torch.save(joint.state_dict(), os.path.join(EXP_ROOT, \"pinn_stageC_joint_final.pth\"))\n",
        "torch.save(joint.flow.state_dict(), os.path.join(EXP_ROOT, \"pinn_stageC_flow_final.pth\"))\n",
        "\n",
        "print(\"Saved final Stage C joint model and flow.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[Stage C] Epoch 001 loss=2.6771\n[Stage C] Epoch 002 loss=2.1746\n[Stage C] Epoch 003 loss=1.8970\n[Stage C] Epoch 004 loss=1.7783\n[Stage C] Epoch 005 loss=2.1459\n[Stage C] Epoch 006 loss=1.7097\n[Stage C] Epoch 007 loss=1.6172\n[Stage C] Epoch 008 loss=1.8677\n[Stage C] Epoch 009 loss=4.0261\n[Stage C] Epoch 010 loss=3.3719\n[Stage C] Epoch 011 loss=7.0045\n[Stage C] Epoch 012 loss=3.2582\n[Stage C] Epoch 013 loss=2.6450\n[Stage C] Epoch 014 loss=2.4671\n[Stage C] Epoch 015 loss=2.0211\n[Stage C] Epoch 016 loss=3.0017\n[Stage C] Epoch 017 loss=1.3341\n[Stage C] Epoch 018 loss=1.2657\n[Stage C] Epoch 019 loss=2.1883\n[Stage C] Epoch 020 loss=4.7274\nSaved final Stage C joint model and flow.\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1766056898267
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Final Evaluation â€” Stage C (memory-safe, paper-ready)\n",
        "\n",
        "import os, json, math, numpy as np, torch\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    roc_auc_score, brier_score_loss\n",
        ")\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Evaluation running on device:\", DEVICE)\n",
        "\n",
        "# ----------------------\n",
        "# Load trained Stage C model\n",
        "# ----------------------\n",
        "joint.eval()\n",
        "joint.flow.eval()\n",
        "\n",
        "# ----------------------\n",
        "# Helpers\n",
        "# ----------------------\n",
        "def metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"rmse\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"r2\":  float(r2_score(y_true, y_pred))\n",
        "    }\n",
        "\n",
        "def gaussian_nll(y, mu, logvar):\n",
        "    logvar = np.clip(logvar, -10.0, 5.0)\n",
        "    var = np.exp(logvar)\n",
        "    return 0.5 * ((y - mu)**2 / var + logvar + np.log(2*np.pi))\n",
        "\n",
        "# ----------------------\n",
        "# Batched forward pass\n",
        "# ----------------------\n",
        "BATCH_EVAL = 2048\n",
        "N = len(X_test_s)\n",
        "\n",
        "mu_m, mu_s = [], []\n",
        "lv_m, lv_s = [], []\n",
        "contexts = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, N, BATCH_EVAL):\n",
        "        xb = torch.tensor(\n",
        "            X_test_s[i:i+BATCH_EVAL],\n",
        "            dtype=torch.float32,\n",
        "            device=DEVICE\n",
        "        )\n",
        "        out = joint(xb)\n",
        "\n",
        "        mu_m.append(out[\"mu_mass\"].cpu().numpy())\n",
        "        mu_s.append(out[\"mu_sfr\"].cpu().numpy())\n",
        "        lv_m.append(out[\"logvar_mass\"].cpu().numpy())\n",
        "        lv_s.append(out[\"logvar_sfr\"].cpu().numpy())\n",
        "        contexts.append(out[\"context\"].cpu())\n",
        "\n",
        "mu_m = np.concatenate(mu_m)\n",
        "mu_s = np.concatenate(mu_s)\n",
        "lv_m = np.concatenate(lv_m)\n",
        "lv_s = np.concatenate(lv_s)\n",
        "contexts = torch.cat(contexts, dim=0)\n",
        "\n",
        "sig_m = np.sqrt(np.exp(lv_m))\n",
        "sig_s = np.sqrt(np.exp(lv_s))\n",
        "\n",
        "# ----------------------\n",
        "# Regression metrics\n",
        "# ----------------------\n",
        "mass_metrics = metrics(yM_test, mu_m)\n",
        "sfr_metrics  = metrics(yS_test, mu_s)\n",
        "\n",
        "nll_mass = float(np.mean(gaussian_nll(yM_test, mu_m, lv_m)))\n",
        "nll_sfr  = float(np.mean(gaussian_nll(yS_test, mu_s, lv_s)))\n",
        "\n",
        "print(\"PINN mass metrics:\", mass_metrics)\n",
        "print(\"PINN sfr metrics :\", sfr_metrics)\n",
        "print(\"Gaussian NLL mass / sfr:\", nll_mass, nll_sfr)\n",
        "\n",
        "# ----------------------\n",
        "# Quenching posterior (flow sampling)\n",
        "# ----------------------\n",
        "N_SAMPLES_Q = 50   # safe on CPU\n",
        "q_means = []\n",
        "q_stds  = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(0, N, BATCH_EVAL):\n",
        "        ctx = contexts[i:i+BATCH_EVAL].to(DEVICE)\n",
        "\n",
        "        # flow returns (S, B, 1)\n",
        "        qs = joint.flow.sample(N_SAMPLES_Q, context=ctx)\n",
        "        qs = qs.squeeze(-1).cpu().numpy()   # (S, B)\n",
        "\n",
        "        # transpose â†’ (B, S)\n",
        "        qs = qs.T\n",
        "\n",
        "        q_means.append(qs.mean(axis=1))\n",
        "        q_stds.append(qs.std(axis=1))\n",
        "\n",
        "q_mean = np.concatenate(q_means, axis=0)   # (N,)\n",
        "q_std  = np.concatenate(q_stds, axis=0)    # (N,)\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Optional classification metrics (if labels exist)\n",
        "# ----------------------\n",
        "q_metrics = {}\n",
        "if \"gmm_train\" in globals():\n",
        "    q_true = gmm_train.predict_proba(\n",
        "        np.vstack([yM_test, yS_test]).T\n",
        "    )[:, quenched_comp]\n",
        "\n",
        "    try:\n",
        "        q_metrics[\"auc\"] = float(roc_auc_score(q_true, q_mean))\n",
        "        q_metrics[\"brier\"] = float(brier_score_loss(q_true, q_mean))\n",
        "        print(\"Q metrics:\", q_metrics)\n",
        "    except Exception as e:\n",
        "        print(\"Q metrics skipped:\", e)\n",
        "\n",
        "# ----------------------\n",
        "# Save evaluation summary\n",
        "# ----------------------\n",
        "summary = {\n",
        "    \"mass_metrics\": mass_metrics,\n",
        "    \"sfr_metrics\": sfr_metrics,\n",
        "    \"nll_mass\": nll_mass,\n",
        "    \"nll_sfr\": nll_sfr,\n",
        "    \"q_metrics\": q_metrics,\n",
        "    \"uncertainty_stats\": {\n",
        "        \"sigma_mass_median\": float(np.median(sig_m)),\n",
        "        \"sigma_sfr_median\":  float(np.median(sig_s)),\n",
        "        \"q_std_median\":      float(np.median(q_std))\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(EXP_ROOT, \"evaluation.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"Saved evaluation.json to\", EXP_ROOT)\n",
        "print(\"Evaluation complete.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Evaluation running on device: cpu\nPINN mass metrics: {'rmse': 0.1873450035963201, 'mae': 0.12495040893554688, 'r2': 0.9290820360183716}\nPINN sfr metrics : {'rmse': 0.3615673819552762, 'mae': 0.2633815407752991, 'r2': 0.7590219378471375}\nGaussian NLL mass / sfr: 18.8544542010777 4.879346867425584\nQ metrics skipped: continuous format is not supported\nSaved evaluation.json to /mnt/batch/tasks/shared/LS_root/mounts/clusters/super-cpu-128gb-e16sv3/code/Users/savsimtws/exp_outputs/run_20251218_111709_a735fda2\nEvaluation complete.\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1766056904370
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.10 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}